# NLP_PapersReview
This is git rep for some of my short reviews on those top NLP papers. If there is anything wrong, please help to indicate it.

##
[x] RoBERTa: A Robustly Optimized BERT Pretraining Approach

[x] BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,
Translation, and Comprehension